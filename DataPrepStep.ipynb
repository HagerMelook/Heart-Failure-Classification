{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Heart Failure Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preparation Step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load dataset from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "data = load_data('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the dataset into features (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_target(df, target_column):\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split dataset into training, validation, and test sets while maintaining class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_validation_test(X, y, train_size=0.7, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(1 - train_size), stratify=y, random_state=random_state\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(test_size / (test_size + val_size)), stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_validation_test(X, y, train_size=0.7, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(1 - train_size), stratify=y, random_state=random_state\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(test_size / (test_size + val_size)), stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print class distributions for training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(y_train, y_val, y_test):\n",
    "    print(\"Training Class Distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "    print(\"Validation Class Distribution:\\n\", y_val.value_counts(normalize=True))\n",
    "    print(\"Test Class Distribution:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform one-hot encoding for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_columns(df, categorical_columns):\n",
    "    return pd.get_dummies(df, columns=categorical_columns, dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standardize numerical features using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(df):\n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
      "0   40   M           ATA        140          289          0     Normal    172   \n",
      "1   49   F           NAP        160          180          0     Normal    156   \n",
      "2   37   M           ATA        130          283          0         ST     98   \n",
      "3   48   F           ASY        138          214          0     Normal    108   \n",
      "4   54   M           NAP        150          195          0     Normal    122   \n",
      "\n",
      "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
      "0              N      0.0       Up             0  \n",
      "1              N      1.0     Flat             1  \n",
      "2              N      0.0       Up             0  \n",
      "3              Y      1.5     Flat             1  \n",
      "4              N      0.0       Up             0  \n",
      "Encoded Columns: Index(['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak',\n",
      "       'Sex_F', 'Sex_M', 'ChestPainType_ASY', 'ChestPainType_ATA',\n",
      "       'ChestPainType_NAP', 'ChestPainType_TA', 'RestingECG_LVH',\n",
      "       'RestingECG_Normal', 'RestingECG_ST', 'ExerciseAngina_N',\n",
      "       'ExerciseAngina_Y', 'ST_Slope_Down', 'ST_Slope_Flat', 'ST_Slope_Up'],\n",
      "      dtype='object')\n",
      "Training Class Distribution:\n",
      " HeartDisease\n",
      "1    0.55296\n",
      "0    0.44704\n",
      "Name: proportion, dtype: float64\n",
      "Validation Class Distribution:\n",
      " HeartDisease\n",
      "1    0.554348\n",
      "0    0.445652\n",
      "Name: proportion, dtype: float64\n",
      "Test Class Distribution:\n",
      " HeartDisease\n",
      "1    0.554348\n",
      "0    0.445652\n",
      "Name: proportion, dtype: float64\n",
      "        Age  RestingBP  Cholesterol  FastingBS     MaxHR   Oldpeak     Sex_F  \\\n",
      "0 -1.433140   0.410909     0.825070  -0.551341  1.382928 -0.832432 -0.515952   \n",
      "1 -0.478484   1.491752    -0.171961  -0.551341  0.754157  0.105664  1.938163   \n",
      "2 -1.751359  -0.129513     0.770188  -0.551341 -1.525138 -0.832432 -0.515952   \n",
      "3 -0.584556   0.302825     0.139040  -0.551341 -1.132156  0.574711  1.938163   \n",
      "4  0.051881   0.951331    -0.034755  -0.551341 -0.581981 -0.832432 -0.515952   \n",
      "\n",
      "      Sex_M  ChestPainType_ASY  ChestPainType_ATA  ChestPainType_NAP  \\\n",
      "0  0.515952          -1.084138           2.075177          -0.532838   \n",
      "1 -1.938163          -1.084138          -0.481887           1.876744   \n",
      "2  0.515952          -1.084138           2.075177          -0.532838   \n",
      "3 -1.938163           0.922392          -0.481887          -0.532838   \n",
      "4  0.515952          -1.084138          -0.481887           1.876744   \n",
      "\n",
      "   ChestPainType_TA  RestingECG_LVH  RestingECG_Normal  RestingECG_ST  \\\n",
      "0         -0.229679       -0.507478           0.814275      -0.490449   \n",
      "1         -0.229679       -0.507478           0.814275      -0.490449   \n",
      "2         -0.229679       -0.507478          -1.228087       2.038947   \n",
      "3         -0.229679       -0.507478           0.814275      -0.490449   \n",
      "4         -0.229679       -0.507478           0.814275      -0.490449   \n",
      "\n",
      "   ExerciseAngina_N  ExerciseAngina_Y  ST_Slope_Down  ST_Slope_Flat  \\\n",
      "0          0.823556         -0.823556      -0.271448      -1.002181   \n",
      "1          0.823556         -0.823556      -0.271448       0.997824   \n",
      "2          0.823556         -0.823556      -0.271448      -1.002181   \n",
      "3         -1.214246          1.214246      -0.271448       0.997824   \n",
      "4          0.823556         -0.823556      -0.271448      -1.002181   \n",
      "\n",
      "   ST_Slope_Up  \n",
      "0     1.150674  \n",
      "1    -0.869056  \n",
      "2     1.150674  \n",
      "3    -0.869056  \n",
      "4     1.150674  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load dataset\n",
    "    df = load_data(\"heart.csv\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Split features and target\n",
    "    X, y = split_features_target(df, 'HeartDisease')\n",
    "    \n",
    "    # Define categorical columns to encode\n",
    "    categorical_columns = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "    df_encoded = encode_categorical_columns(X, categorical_columns)\n",
    "    print(\"Encoded Columns:\", df_encoded.columns)\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_train_validation_test(df_encoded, y)\n",
    "    \n",
    "    # Validate class distribution\n",
    "    print_class_distribution(y_train, y_val, y_test)\n",
    "        \n",
    "    # if needed, Standardize dataset\n",
    "    df_standardized = standardize_features(df_encoded)\n",
    "    print(df_standardized.head())\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train => correct : 549, training set accuracy: 0.8551401869158879\n",
      "Validation => correct : 80, training set accuracy: 0.8695652173913043\n",
      "Test => correct : 155, training set accuracy: 0.842391304347826\n"
     ]
    }
   ],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    class Node:\n",
    "        def __init__(\n",
    "            self,\n",
    "            feature : int  | None = None,\n",
    "            threshold: float | None = None,\n",
    "            predicted_class: int | None  = None,\n",
    "            depth: int | None = None,\n",
    "            left: \"Node | None\" = None,\n",
    "            right: \"Node | None\" = None,\n",
    "        ):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold  # <= threshold goes to left, > threshold goes to right\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.predicted_class = predicted_class\n",
    "            self.depth = depth\n",
    "\n",
    "    def __init__(self, max_depth: int = 4, min_samples_split: int =10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X : pd.DataFrame, y : pd.Series):\n",
    "        self.root = self._build_decision_tree(X, y, 0)\n",
    "\n",
    "    def predict(self, X : pd.DataFrame) -> list[int]: # classification of each input\n",
    "        predictions_list = [ self._traverse_tree(row, self.root) for _, row in X.iterrows() ]\n",
    "        return predictions_list\n",
    "\n",
    "    def _build_decision_tree(self, X : pd.DataFrame, y : pd.Series, curr_depth : int) -> Node:\n",
    "        if (curr_depth == self.max_depth or len(set(y)) == 1 or self.min_samples_split > len(y)):\n",
    "            return self.Node(predicted_class=Counter(y).most_common(1)[0][0], depth=curr_depth)\n",
    "\n",
    "        feature, threshold = self._best_split(X, y)\n",
    "        tree_node = self.Node(feature= feature,threshold=threshold,depth=curr_depth)\n",
    "\n",
    "        X_left_split = X.loc[X.iloc[:, feature] <= threshold, :]\n",
    "        y_left_split = y[X.iloc[:, feature] <= threshold]\n",
    "        X_right_split = X.loc[X.iloc[:, feature] > threshold, :]\n",
    "        y_right_split = y[X.iloc[:, feature] > threshold]\n",
    "\n",
    "        tree_node.left = self._build_decision_tree(X_left_split, y_left_split, curr_depth + 1)\n",
    "        tree_node.right = self._build_decision_tree(X_right_split, y_right_split, curr_depth + 1)\n",
    "        return tree_node\n",
    "\n",
    "    def _best_split(self, X : pd.DataFrame, y : pd.Series) -> tuple: # feature and thresold are integers\n",
    "        unique_values_list = [np.unique(X.iloc[:, i].values) for i in range(X.shape[1])]\n",
    "        mid_points_list = [values[:-1] + np.diff(values) / 2 for values in unique_values_list]\n",
    "\n",
    "        max_info_gain = {\"infoGain\": -float(\"inf\"), \"feature\": -1, \"threshold\": None}\n",
    "        for i in range(X.shape[1]):\n",
    "            local_max_info_gain = { \"infoGain\": -float(\"inf\"), \"feature\": -1, \"threshold\": None}\n",
    "            for split in mid_points_list[i]:\n",
    "                left_split = y[X.iloc[:, i] <= split]\n",
    "                right_split = y[X.iloc[:, i] > split]\n",
    "                info_gain = self._information_gain(y, left_split, right_split)\n",
    "                if info_gain > local_max_info_gain[\"infoGain\"]:\n",
    "                    local_max_info_gain = { \"infoGain\": info_gain, \"feature\": i, \"threshold\": split}\n",
    "            if local_max_info_gain[\"infoGain\"] > max_info_gain[\"infoGain\"]:\n",
    "                max_info_gain = local_max_info_gain\n",
    "\n",
    "        return max_info_gain[\"feature\"], max_info_gain[\"threshold\"]\n",
    "\n",
    "    def _traverse_tree(self, x : pd.Series, node : Node) -> int: # returns a class\n",
    "        if node.predicted_class is not None:\n",
    "            return node.predicted_class\n",
    "        return self._traverse_tree(x, node.left) if x[node.feature] <= node.threshold else self._traverse_tree(x, node.right)\n",
    "\n",
    "    def _information_gain(self, y : pd.Series, y1 : pd.Series, y2 : pd.Series) -> float:\n",
    "        y_entropy = self._entropy(y)\n",
    "        y_entropy_after_split_1 = self._entropy(y1)\n",
    "        y_entropy_after_split_2 = self._entropy(y2)\n",
    "        return (\n",
    "            y_entropy\n",
    "            - (len(y1) / len(y)) * y_entropy_after_split_1\n",
    "            - (len(y2) / len(y)) * y_entropy_after_split_2\n",
    "        )\n",
    "\n",
    "    def _entropy(self, y : pd.Series) -> float | int:\n",
    "        zeroes_number = (y == 0).sum()\n",
    "        ones_number = (y == 1).sum()\n",
    "        probs = np.array([ones_number / len(y), zeroes_number / len(y)])\n",
    "        return -np.sum(probs * np.log2(np.where(probs == 0, 1, probs)))\n",
    "\n",
    "\n",
    "cls = DecisionTreeClassifier()\n",
    "cls.fit(X_train, y_train)\n",
    "predict_list = cls.predict(X_train)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(predict_list)):\n",
    "    if predict_list[i] == y_train.iloc[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Train => correct : {correct}, training set accuracy: {correct / len(y_train)}\")\n",
    "\n",
    "predict_list = cls.predict(X_val)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(predict_list)):\n",
    "    if predict_list[i] == y_val.iloc[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(\n",
    "    f\"Validation => correct : {correct}, training set accuracy: {correct / len(y_val)}\"\n",
    ")\n",
    "\n",
    "predict_list = cls.predict(X_test)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(predict_list)):\n",
    "    if predict_list[i] == y_test.iloc[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Test => correct : {correct}, training set accuracy: {correct / len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing the model to make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,8))\n",
    "# sns.heatmap(df_encoded.corr(), annot=True, fmt='.1f')\n",
    "# df_encoded.hist(figsize=(20,20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
